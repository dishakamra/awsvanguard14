{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T16:22:15.964519Z",
     "iopub.status.busy": "2025-09-16T16:22:15.963948Z",
     "iopub.status.idle": "2025-09-16T16:22:16.109276Z",
     "shell.execute_reply": "2025-09-16T16:22:16.108366Z",
     "shell.execute_reply.started": "2025-09-16T16:22:15.964487Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "What design patterns are used in this code?\n",
    "\n",
    "You are an expert Python developer. Here's a codebase context:\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.data = []\n",
    "    \n",
    "    def load_data(self, source):\n",
    "        # Complex data loading logic\n",
    "        pass\n",
    "    \n",
    "    def transform_data(self, transformations):\n",
    "        # Data transformation pipeline\n",
    "        pass\n",
    "    \n",
    "    def validate_data(self, rules):\n",
    "        # Data validation logic\n",
    "        pass\n",
    "\n",
    "This class is part of a larger system with 50+ similar classes.\n",
    "Always reference this context when answering questions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking cross-region inference profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"1e1185b1-1e0d-45dc-afed-ed45cb8bf80c\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Tue, 30 Sep 2025 20:11:50 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"3892\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"1e1185b1-1e0d-45dc-afed-ed45cb8bf80c\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  },\n",
      "  \"output\": {\n",
      "    \"message\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"text\": \"Given the provided context, let's analyze the design patterns that might be used in the `DataProcessor` class and the larger system with 50+ similar classes.\\n\\n### Design Patterns in the `DataProcessor` Class\\n\\n1. **Abstract Factory Pattern**:\\n   - If the system creates families of related objects without specifying their concrete classes, an Abstract Factory Pattern might be used. However, this pattern is not directly evident from the provided snippet.\\n\\n2. **Builder Pattern**:\\n   - If the `DataProcessor` class is being constructed in a complex manner with multiple steps, the Builder Pattern might be used. However, the `__init__` method here is straightforward and doesn't suggest this pattern.\\n\\n3. **Singleton Pattern**:\\n   - If the `DataProcessor` class is intended to have only one instance throughout the application, the Singleton Pattern might be used. This pattern is not evident from the provided snippet.\\n\\n4. **Strategy Pattern**:\\n   - If the `DataProcessor` class uses different algorithms for data loading, transformation, and validation that can be interchanged at runtime, the Strategy Pattern might be used. The methods `load_data`, `transform_data`, and `validate_data` could potentially use this pattern internally.\\n\\n5. **Template Method Pattern**:\\n   - If the `DataProcessor` class defines a skeleton of an algorithm in a method, deferring some steps to subclasses, the Template Method Pattern might be used. The provided methods could be part of a template method that defines the overall data processing pipeline.\\n\\n6. **Facade Pattern**:\\n   - If the `DataProcessor` class provides a simplified interface to a complex subsystem of classes, the Facade Pattern might be used. This pattern is not explicitly evident from the provided snippet.\\n\\n7. **Chain of Responsibility Pattern**:\\n   - If the `DataProcessor` class passes requests along a chain of handlers, the Chain of Responsibility Pattern might be used. This pattern might be used internally within the `transform_data` or `validate_data` methods.\\n\\n### Design Patterns in the Larger System\\n\\nGiven that there are 50+ similar classes, it's likely that the system uses several design patterns to manage complexity and ensure maintainability.\\n\\n1. **Factory Pattern**:\\n   - To manage the creation of similar objects, the Factory Pattern (including its variants like Abstract Factory, Factory Method) might be used.\\n\\n2. **Decorator Pattern**:\\n   - To add responsibilities to objects dynamically, the Decorator Pattern might be used. This pattern is often used to extend the functionality of the `DataProcessor` classes.\\n\\n3. **Composite Pattern**:\\n   - If the system needs to treat individual objects and compositions of objects uniformly, the Composite Pattern might be used.\\n\\n4. **Observer Pattern**:\\n   - If the system needs to notify certain objects when the state of another object changes, the Observer Pattern might be used.\\n\\n5. **Prototype Pattern**:\\n   - If the system needs to create new objects by copying existing ones, the Prototype Pattern might be used.\\n\\n6. **Adapter Pattern**:\\n   - If the system needs to make existing classes work with others that have incompatible interfaces, the Adapter Pattern might be used.\\n\\n### Conclusion\\n\\nBased on the provided context, the `DataProcessor` class itself might be using the **Strategy Pattern** or **Template Method Pattern** internally. For the larger system, patterns like **Factory Pattern**, **Decorator Pattern**, **Composite Pattern**, **Observer Pattern**, **Prototype Pattern**, and **Adapter Pattern** might be used to manage the complexity and ensure maintainability across the 50+ similar classes.\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"stopReason\": \"end_turn\",\n",
      "  \"usage\": {\n",
      "    \"inputTokens\": 132,\n",
      "    \"outputTokens\": 739,\n",
      "    \"totalTokens\": 871\n",
      "  },\n",
      "  \"metrics\": {\n",
      "    \"latencyMs\": 4735\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = bedrock.converse(\n",
    "    modelId='us.amazon.nova-lite-v1:0',\n",
    "    messages=[\n",
    "       {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "          ]\n",
    "      }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use your own inference profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"56d89816-d4d4-4a2c-bac3-a056bdf15d10\",\n",
      "    \"HTTPStatusCode\": 201,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Tue, 30 Sep 2025 20:12:33 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"125\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"56d89816-d4d4-4a2c-bac3-a056bdf15d10\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  },\n",
      "  \"inferenceProfileArn\": \"arn:aws:bedrock:us-east-1:058264218236:application-inference-profile/b3x58b3qiuhe\",\n",
      "  \"status\": \"ACTIVE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# create an inference profile\n",
    "bedrock_control = boto3.client('bedrock', region_name='us-east-1')\n",
    "\n",
    "response = bedrock_control.create_inference_profile(\n",
    "    inferenceProfileName='my-nova-lite-profile',\n",
    "    description='Custom inference profile for Nova Lite',\n",
    "    modelSource={\n",
    "        'copyFrom': 'arn:aws:bedrock:us-east-1:058264218236:inference-profile/us.amazon.nova-lite-v1:0'\n",
    "    }\n",
    ")\n",
    "inference_profile_arn = response['inferenceProfileArn']\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"3bdf24df-a88e-4bdc-833f-8ff3821510e7\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Tue, 30 Sep 2025 20:13:10 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"4466\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"3bdf24df-a88e-4bdc-833f-8ff3821510e7\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  },\n",
      "  \"output\": {\n",
      "    \"message\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"text\": \"Given the provided code snippet and the context that this class is part of a larger system with 50+ similar classes, let's analyze the design patterns that might be used in this codebase.\\n\\n### Design Patterns in the Provided Code\\n\\n1. **Factory Pattern**:\\n   - **Context**: The `DataProcessor` class is initialized with a `config` object. This suggests that there might be a factory method or class responsible for creating instances of `DataProcessor` and possibly other similar classes.\\n   - **Rationale**: A factory pattern can help manage the creation of these objects, ensuring that they are properly configured and initialized.\\n\\n2. **Strategy Pattern**:\\n   - **Context**: The `load_data`, `transform_data`, and `validate_data` methods suggest that different strategies or algorithms can be applied to process the data.\\n   - **Rationale**: The Strategy Pattern allows these methods to be interchangeable at runtime. For example, different data loading strategies, transformation pipelines, or validation rules can be plugged in.\\n\\n3. **Template Method Pattern**:\\n   - **Context**: The `DataProcessor` class has a defined structure with methods like `load_data`, `transform_data`, and `validate_data`. These methods can be considered as template methods that define the skeleton of an algorithm.\\n   - **Rationale**: The Template Method Pattern allows subclasses to redefine certain steps of an algorithm without changing the algorithm's overall structure. This can be useful if different subclasses of `DataProcessor` need to customize certain steps.\\n\\n4. **Singleton Pattern**:\\n   - **Context**: If the `config` object is a shared resource across multiple instances of `DataProcessor` and other similar classes.\\n   - **Rationale**: The Singleton Pattern ensures that a class has only one instance and provides a global point of access to it. This can be useful for configuration objects that need to be consistent across the system.\\n\\n5. **Chain of Responsibility Pattern**:\\n   - **Context**: If the data transformation and validation processes involve a sequence of steps that need to be executed in a specific order.\\n   - **Rationale**: The Chain of Responsibility Pattern allows a request to be passed along a chain of handlers. Each handler decides either to process the request or to pass it to the next handler in the chain.\\n\\n### Example Implementation of Patterns\\n\\nHere's a brief example of how these patterns might be implemented:\\n\\n```python\\nclass DataProcessor:\\n    def __init__(self, config):\\n        self.config = config\\n        self.data = []\\n\\n    def load_data(self, source):\\n        # Complex data loading logic\\n        pass\\n\\n    def transform_data(self, transformations):\\n        # Data transformation pipeline\\n        for transformation in transformations:\\n            self.data = transformation.apply(self.data)\\n\\n    def validate_data(self, rules):\\n        # Data validation logic\\n        for rule in rules:\\n            if not rule.check(self.data):\\n                raise ValueError(\\\"Data validation failed\\\")\\n\\nclass Transformation:\\n    def apply(self, data):\\n        # Apply transformation logic\\n        pass\\n\\nclass ValidationRule:\\n    def check(self, data):\\n        # Check validation logic\\n        pass\\n\\nclass DataProcessorFactory:\\n    @staticmethod\\n    def create_data_processor(config):\\n        return DataProcessor(config)\\n\\n# Usage\\nconfig = Config()  # Assume Config is a class that holds configuration details\\nprocessor = DataProcessorFactory.create_data_processor(config)\\nprocessor.load_data(source)\\nprocessor.transform_data([Transformation1(), Transformation2()])\\nprocessor.validate_data([ValidationRule1(), ValidationRule2()])\\n```\\n\\n### Summary\\n\\n- **Factory Pattern**: For creating instances of `DataProcessor`.\\n- **Strategy Pattern**: For interchangeable data loading, transformation, and validation logic.\\n- **Template Method Pattern**: For defining the skeleton of the data processing algorithm.\\n- **Singleton Pattern**: For managing shared configuration objects.\\n- **Chain of Responsibility Pattern**: For handling sequences of transformations and validations.\\n\\nThese patterns help in making the codebase more modular, maintainable, and extensible.\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"stopReason\": \"end_turn\",\n",
      "  \"usage\": {\n",
      "    \"inputTokens\": 132,\n",
      "    \"outputTokens\": 849,\n",
      "    \"totalTokens\": 981\n",
      "  },\n",
      "  \"metrics\": {\n",
      "    \"latencyMs\": 4262\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = bedrock.converse(\n",
    "    modelId=inference_profile_arn,\n",
    "    messages=[\n",
    "       {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "          ]\n",
    "      }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_control.delete_inference_profile(\n",
    "    inferenceProfileIdentifier=inference_profile_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload jsonl file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"batch-data-inference\"\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Upload manifest to S3\n",
    "input_key = 'input/city-reviews-manifest.jsonl'\n",
    "s3.upload_file('input/city-reviews-manifest.jsonl', bucket_name, input_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the CreateModelInvocationJob API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "accountnumber = \"058264218236\"\n",
    "\n",
    "# Create the bedrock control plane client\n",
    "bedrock_control = boto3.client('bedrock', region_name='us-east-1')\n",
    "\n",
    "# Create batch inference job\n",
    "job_name = f\"batch-job-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "response = bedrock_control.create_model_invocation_job(\n",
    "    jobName=job_name,\n",
    "    roleArn=f\"arn:aws:iam::{accountnumber}:role/BedrockBatchRole\",\n",
    "    modelId=\"us.amazon.nova-lite-v1:0\",\n",
    "    inputDataConfig={\n",
    "        's3InputDataConfig': {\n",
    "            's3Uri': f's3://{bucket_name}/input/'\n",
    "        }\n",
    "    },\n",
    "    outputDataConfig={\n",
    "        's3OutputDataConfig': {\n",
    "            's3Uri': f's3://{bucket_name}/output/'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "job_arn = response['jobArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: Validating\n"
     ]
    }
   ],
   "source": [
    "status_response = bedrock_control.get_model_invocation_job(jobIdentifier=job_arn)\n",
    "status = status_response['status']\n",
    "print(f\"Job status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of waiting, let's use a jobArn for a job we know already finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_job_arn = \"arn:aws:bedrock:us-east-1:058264218236:model-invocation-job/p2t64s4pml6t\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the job to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: Completed\n"
     ]
    }
   ],
   "source": [
    "# Wait for job completion\n",
    "while True:\n",
    "    status_response = bedrock_control.get_model_invocation_job(jobIdentifier=completed_job_arn)\n",
    "    status = status_response['status']\n",
    "    print(f\"Job status: {status}\")\n",
    "    \n",
    "    if status in ['Completed', 'PartiallyCompleted', 'Failed', 'Stopped', 'Expired']:\n",
    "        break\n",
    "    \n",
    "    time.sleep(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: batch-output/city-reviews-manifest.jsonl.out\n",
      "Downloaded: batch-output/manifest.json.out\n",
      "Downloaded: batch-output/city-reviews-manifest.jsonl.out\n",
      "Downloaded: batch-output/manifest.json.out\n"
     ]
    }
   ],
   "source": [
    "if status in [\"Completed\", \"PartiallyCompleted\"]:\n",
    "    # Download results\n",
    "    output_objects = s3.list_objects_v2(Bucket=bucket_name, Prefix='output/')\n",
    "    \n",
    "    os.makedirs('batch-output', exist_ok=True)\n",
    "    \n",
    "    for obj in output_objects.get('Contents', []):\n",
    "        if obj['Key'].endswith('/'):\n",
    "            continue\n",
    "            \n",
    "        local_file = os.path.join('batch-output', obj['Key'].split('/')[-1])\n",
    "        s3.download_file(bucket_name, obj['Key'], local_file)\n",
    "        print(f\"Downloaded: {local_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the job we created to save on cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_control.stop_model_invocation_job(\n",
    "    jobIdentifier=job_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Caching with Nova Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T16:22:16.995849Z",
     "iopub.status.busy": "2025-09-16T16:22:16.995572Z",
     "iopub.status.idle": "2025-09-16T16:22:16.999265Z",
     "shell.execute_reply": "2025-09-16T16:22:16.998587Z",
     "shell.execute_reply.started": "2025-09-16T16:22:16.995828Z"
    }
   },
   "outputs": [],
   "source": [
    "# Large context that we want to cache\n",
    "cached_context = \"\"\"\n",
    "You are a expert Python developer. Here's a large codebase context:\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.data = []\n",
    "    \n",
    "    def load_data(self, source):\n",
    "        # Complex data loading logic\n",
    "        pass\n",
    "    \n",
    "    def transform_data(self, transformations):\n",
    "        # Data transformation pipeline\n",
    "        pass\n",
    "    \n",
    "    def validate_data(self, rules):\n",
    "        # Data validation logic\n",
    "        pass\n",
    "\n",
    "This class is part of a larger system with 50+ similar classes.\n",
    "Always reference this context when answering questions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First request that establishes the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T16:22:17.747824Z",
     "iopub.status.busy": "2025-09-16T16:22:17.747554Z",
     "iopub.status.idle": "2025-09-16T16:22:21.201240Z",
     "shell.execute_reply": "2025-09-16T16:22:21.200270Z",
     "shell.execute_reply.started": "2025-09-16T16:22:17.747802Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"8e7465ae-fa24-4a5d-9b98-81d13ce554b9\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Tue, 30 Sep 2025 20:26:06 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"3682\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"8e7465ae-fa24-4a5d-9b98-81d13ce554b9\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  },\n",
      "  \"output\": {\n",
      "    \"message\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"text\": \"Given the provided context, the `DataProcessor` class exhibits several design patterns. Here are some of the most notable ones:\\n\\n### 1. **Factory Pattern**\\nWhile not explicitly shown in the provided code, it's common to use a Factory Pattern to create instances of `DataProcessor` and similar classes. This pattern can help manage the creation of objects and ensure that the correct configuration is applied.\\n\\n### 2. **Builder Pattern**\\nThe `__init__` method of `DataProcessor` takes a `config` parameter, which suggests that the Builder Pattern might be used to construct complex objects step-by-step. This pattern is useful when you have a class with many parameters that can be set in various configurations.\\n\\n### 3. **Strategy Pattern**\\nThe methods `load_data`, `transform_data`, and `validate_data` suggest the use of the Strategy Pattern, where the algorithms for loading, transforming, and validating data can be encapsulated and interchanged. This allows for flexibility in how data is processed.\\n\\n### 4. **Template Method Pattern**\\nThe skeleton of the `DataProcessor` class, with placeholder methods for `load_data`, `transform_data`, and `validate_data`, suggests the Template Method Pattern. This pattern defines the skeleton of an algorithm in a method, deferring some steps to subclasses. Subclasses can override the steps without changing the overall algorithm's structure.\\n\\n### 5. **Singleton Pattern**\\nAlthough not explicitly shown, it's common to use the Singleton Pattern for the `config` object. This ensures that there is only one configuration instance throughout the application, which can be useful for maintaining consistency across multiple instances of `DataProcessor`.\\n\\n### 6. **Chain of Responsibility Pattern**\\nIf the transformations and validations are modular and can be chained together, the Chain of Responsibility Pattern might be used. This pattern allows multiple objects to handle a request without specifying the receiver explicitly, passing the request along the chain until it is handled.\\n\\n### 7. **Observer Pattern**\\nIf the `DataProcessor` class needs to notify other components when data is loaded, transformed, or validated, the Observer Pattern could be used. This pattern defines a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically.\\n\\n### 8. **Decorator Pattern**\\nIf you need to add additional responsibilities to `DataProcessor` objects dynamically, the Decorator Pattern might be used. This pattern allows behavior to be added to individual objects, either statically or dynamically, without affecting the behavior of other objects from the same class.\\n\\n### Summary\\n- **Factory Pattern**: For object creation.\\n- **Builder Pattern**: For constructing complex objects.\\n- **Strategy Pattern**: For interchangeable algorithms.\\n- **Template Method Pattern**: For defining the skeleton of an algorithm.\\n- **Singleton Pattern**: For a single configuration instance.\\n- **Chain of Responsibility Pattern**: For modular request handling.\\n- **Observer Pattern**: For notifying other components.\\n- **Decorator Pattern**: For adding responsibilities dynamically.\\n\\nThese patterns help in making the code more modular, maintainable, and scalable, which is crucial for a large codebase with many similar classes.\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"stopReason\": \"end_turn\",\n",
      "  \"usage\": {\n",
      "    \"inputTokens\": 9,\n",
      "    \"outputTokens\": 649,\n",
      "    \"totalTokens\": 782,\n",
      "    \"cacheReadInputTokens\": 0,\n",
      "    \"cacheWriteInputTokens\": 124\n",
      "  },\n",
      "  \"metrics\": {\n",
      "    \"latencyMs\": 2913\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "firstRequest = bedrock.converse(\n",
    "    modelId='amazon.nova-lite-v1:0',\n",
    "    messages=[\n",
    "       {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": cached_context\n",
    "                },\n",
    "                {\n",
    "                    \"cachePoint\": {\n",
    "                        \"type\": \"default\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": \"What design patterns are used in this code?\"\n",
    "                }\n",
    "          ]\n",
    "      }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(json.dumps(firstRequest, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second request that uses cached context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T16:22:31.563761Z",
     "iopub.status.busy": "2025-09-16T16:22:31.563325Z",
     "iopub.status.idle": "2025-09-16T16:22:36.229709Z",
     "shell.execute_reply": "2025-09-16T16:22:36.229038Z",
     "shell.execute_reply.started": "2025-09-16T16:22:31.563737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"fe7ca182-413c-4bb9-8ffa-27212baa9eb8\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Tue, 30 Sep 2025 20:27:31 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"6236\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"fe7ca182-413c-4bb9-8ffa-27212baa9eb8\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  },\n",
      "  \"output\": {\n",
      "    \"message\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"text\": \"Certainly! Let's break down the provided code and discuss some aspects of it, considering the context of a larger system with 50+ similar classes.\\n\\n### Class Overview\\n\\nThe `DataProcessor` class is designed to handle data processing tasks, including loading, transforming, and validating data. Here's a more detailed look at each component:\\n\\n1. **Initialization (`__init__`)**:\\n    ```python\\n    def __init__(self, config):\\n        self.config = config\\n        self.data = []\\n    ```\\n    - **`config`**: This parameter suggests that the class is configured through a configuration object. This allows for flexibility and reusability across different instances of the class.\\n    - **`data`**: An empty list that will be populated with data after it is loaded.\\n\\n2. **Loading Data (`load_data`)**:\\n    ```python\\n    def load_data(self, source):\\n        # Complex data loading logic\\n        pass\\n    ```\\n    - **`source`**: This parameter indicates where the data is being loaded from. It could be a file path, a database connection, a web service, etc.\\n    - **Complex data loading logic**: This placeholder comment suggests that the actual implementation will involve intricate steps to fetch and possibly preprocess the data.\\n\\n3. **Transforming Data (`transform_data`)**:\\n    ```python\\n    def transform_data(self, transformations):\\n        # Data transformation pipeline\\n        pass\\n    ```\\n    - **`transformations`**: This parameter implies that the data will be transformed according to a set of rules or functions provided.\\n    - **Data transformation pipeline**: This placeholder comment indicates that there is a sequence of transformations applied to the data.\\n\\n4. **Validating Data (`validate_data`)**:\\n    ```python\\n    def validate_data(self, rules):\\n        # Data validation logic\\n        pass\\n    ```\\n    - **`rules`**: This parameter suggests that the data will be checked against a set of criteria or rules to ensure its integrity and correctness.\\n    - **Data validation logic**: This placeholder comment indicates that there is a set of checks applied to the data to validate it.\\n\\n### Considerations for a Larger System\\n\\nGiven that this class is part of a larger system with 50+ similar classes, here are some considerations and best practices:\\n\\n1. **Consistency**:\\n    - Ensure that all similar classes follow a consistent design pattern. This makes it easier for developers to understand and maintain the codebase.\\n    - Use consistent naming conventions and parameter types.\\n\\n2. **Configuration Management**:\\n    - Since the class uses a `config` object, ensure that the configuration management system is robust and scalable. This might involve using a centralized configuration service or a well-defined configuration file format.\\n\\n3. **Error Handling**:\\n    - Implement robust error handling in methods like `load_data`, `transform_data`, and `validate_data`. This could involve logging errors, raising exceptions, or providing fallback mechanisms.\\n\\n4. **Modularity and Reusability**:\\n    - Break down complex logic into smaller, reusable functions or classes. This makes the code easier to test and maintain.\\n    - Consider creating a base class with common functionality that can be inherited by other data processing classes.\\n\\n5. **Testing**:\\n    - Write comprehensive unit tests and integration tests for each method. This ensures that the class behaves as expected under various conditions.\\n    - Use mocking for dependencies like data sources and transformation rules to isolate the tests.\\n\\n6. **Documentation**:\\n    - Provide clear and detailed documentation for each method, including parameters, return values, and potential exceptions.\\n    - Consider using docstrings to maintain documentation within the codebase.\\n\\n7. **Performance**:\\n    - Profile the data loading and transformation methods to identify and address bottlenecks.\\n    - Consider using parallel processing or asynchronous I/O operations to improve performance.\\n\\n8. **Scalability**:\\n    - Design the class to handle large datasets efficiently. This might involve using generators, streaming data, or distributed processing frameworks.\\n\\n### Example Enhancements\\n\\nHere's a small example of how you might enhance the `DataProcessor` class by adding some of these considerations:\\n\\n```python\\nimport logging\\n\\nclass DataProcessor:\\n    def __init__(self, config):\\n        self.config = config\\n        self.data = []\\n        self.logger = logging.getLogger(__name__)\\n\\n    def load_data(self, source):\\n        try:\\n            # Complex data loading logic\\n            self.data = self._load_from_source(source)\\n        except Exception as e:\\n            self.logger.error(f\\\"Failed to load data from {source}: {e}\\\")\\n            raise\\n\\n    def _load_from_source(self, source):\\n        # Placeholder for actual data loading logic\\n        pass\\n\\n    def transform_data(self, transformations):\\n        try:\\n            # Data transformation pipeline\\n            self.data = self._apply_transformations(transformations)\\n        except Exception as e:\\n            self.logger.error(f\\\"Failed to transform data: {e}\\\")\\n            raise\\n\\n    def _apply_transformations(self, transformations):\\n        # Placeholder for actual transformation logic\\n        pass\\n\\n    def validate_data(self, rules):\\n        try:\\n            # Data validation logic\\n            self._validate(rules)\\n        except Exception as e:\\n            self.logger.error(f\\\"Failed to validate data: {e}\\\")\\n            raise\\n\\n    def _validate(self, rules):\\n        # Placeholder for actual validation logic\\n        pass\\n```\\n\\nIn this example, I've added a logger for error handling and encapsulated the complex logic within private methods. This helps to keep the public interface clean and makes it easier to manage exceptions and logging.\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"stopReason\": \"end_turn\",\n",
      "  \"usage\": {\n",
      "    \"inputTokens\": 10,\n",
      "    \"outputTokens\": 1200,\n",
      "    \"totalTokens\": 1334,\n",
      "    \"cacheReadInputTokens\": 124,\n",
      "    \"cacheWriteInputTokens\": 0\n",
      "  },\n",
      "  \"metrics\": {\n",
      "    \"latencyMs\": 4491\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "secondRequest = bedrock.converse(\n",
    "    modelId='amazon.nova-lite-v1:0',\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": cached_context\n",
    "                },\n",
    "                {\n",
    "                    \"cachePoint\": {\n",
    "                        \"type\": \"default\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": \"what else can you tell me about this code?\"\n",
    "                }\n",
    "          ]\n",
    "      }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(json.dumps(secondRequest, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### First request ###\n",
      "Usage: {\n",
      "  \"inputTokens\": 9,\n",
      "  \"outputTokens\": 649,\n",
      "  \"totalTokens\": 782,\n",
      "  \"cacheReadInputTokens\": 0,\n",
      "  \"cacheWriteInputTokens\": 124\n",
      "}\n",
      "Metrics: {\n",
      "  \"latencyMs\": 2913\n",
      "}\n",
      "\n",
      "\n",
      "### Second request ###\n",
      "Usage: {\n",
      "  \"inputTokens\": 10,\n",
      "  \"outputTokens\": 1200,\n",
      "  \"totalTokens\": 1334,\n",
      "  \"cacheReadInputTokens\": 124,\n",
      "  \"cacheWriteInputTokens\": 0\n",
      "}\n",
      "Metrics: {\n",
      "  \"latencyMs\": 4491\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"### First request ###\")\n",
    "print(\"Usage:\", json.dumps(firstRequest['usage'], indent=2))\n",
    "print(\"Metrics:\", json.dumps(firstRequest['metrics'], indent=2))\n",
    "\n",
    "print(\"\\n\\n### Second request ###\")\n",
    "print(\"Usage:\", json.dumps(secondRequest['usage'], indent=2))\n",
    "print(\"Metrics:\", json.dumps(secondRequest['metrics'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converse on documents\n",
    "### Using `bytes` as a source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input/AnyCompany_financial_10K.pdf', \"rb\") as file:\n",
    "    doc_bytes = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock.converse(\n",
    "    modelId='us.amazon.nova-lite-v1:0',\n",
    "    messages=[\n",
    "       {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"document\": {\n",
    "                        \"format\": \"pdf\",\n",
    "                        \"name\": \"AnyCompany_financial\",\n",
    "                        \"source\": {\n",
    "                            \"bytes\": doc_bytes\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": \"What investments have AnyCompany made?\"\n",
    "                }\n",
    "          ]\n",
    "      }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using s3Location as a source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"batch-data-inference\"\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "input_key = 'AnyCompany_financial_10K.pdf'\n",
    "s3.upload_file('input/AnyCompany_financial_10K.pdf', bucket_name, input_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock.converse(\n",
    "    modelId='us.amazon.nova-lite-v1:0',\n",
    "    messages=[\n",
    "       {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"document\": {\n",
    "                        \"format\": \"pdf\",\n",
    "                        \"name\": \"AnyCompany_financial\",\n",
    "                        \"source\": {\n",
    "                            \"s3Location\": {\n",
    "                                \"uri\": f\"s3://{bucket_name}/{input_key}\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": \"What property does AnyCompany own?\"\n",
    "                }\n",
    "          ]\n",
    "      }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the DynamoDB Table and set the time to live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"conversation-history\"\n",
    "\n",
    "dynamodb = boto3.client('dynamodb', region_name=\"us-east-1\")\n",
    "response = dynamodb.create_table(\n",
    "    TableName=table_name,\n",
    "    BillingMode='PAY_PER_REQUEST',\n",
    "    AttributeDefinitions=[\n",
    "        {\n",
    "            'AttributeName': 'userId',\n",
    "            'AttributeType': 'S'\n",
    "        },\n",
    "        {\n",
    "            'AttributeName': 'timestamp',\n",
    "            'AttributeType': 'N'\n",
    "        }\n",
    "    ],\n",
    "    KeySchema=[\n",
    "        {\n",
    "            'AttributeName': 'userId',\n",
    "            'KeyType': 'HASH'\n",
    "        },\n",
    "        {\n",
    "            'AttributeName': 'timestamp',\n",
    "            'KeyType': 'RANGE'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Wait for table to be active\n",
    "waiter = dynamodb.get_waiter('table_exists')\n",
    "waiter.wait(TableName=table_name)\n",
    "\n",
    "ttl_response = dynamodb.update_time_to_live(\n",
    "    TableName=table_name,\n",
    "    TimeToLiveSpecification={\n",
    "        'AttributeName': 'ttl',\n",
    "        'Enabled': True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the functions to get the conversation history and store new messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"conversation-history\"\n",
    "ddbclient = boto3.client('dynamodb', region_name = \"us-east-1\")\n",
    "\n",
    "def get_conversation_history(user_id):\n",
    "    # Pagination is required in case the conversation is more than 1MB.\n",
    "    paginator = ddbclient.get_paginator('query')\n",
    "    pages = paginator.paginate(\n",
    "        TableName = table_name,\n",
    "        KeyConditionExpression = 'userId = :val',\n",
    "        ExpressionAttributeValues = {':val': {'S': user_id}}\n",
    "    )\n",
    "    messages = []\n",
    "    for page in pages:\n",
    "        for item in page.get('Items', []):\n",
    "            messages.append({\n",
    "                \"role\": item[\"role\"][\"S\"],\n",
    "                \"content\": [{ \"text\": item[\"message\"][\"S\"] }]\n",
    "            })\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamodbResource = boto3.resource('dynamodb', region_name = \"us-east-1\")\n",
    "conversation_table = dynamodbResource.Table(table_name)\n",
    "\n",
    "def store_message(user_id, message, role):\n",
    "    now_in_seconds = int(time.time())\n",
    "    expire_ttl = now_in_seconds + (1 * 24 * 60 * 60) # 1 day\n",
    "    \n",
    "    conversation_table.put_item(\n",
    "        Item = {\n",
    "            'userId': user_id,\n",
    "            'timestamp': now_in_seconds,\n",
    "            'message': message,\n",
    "            'role': role,\n",
    "            'ttl': expire_ttl\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the function to send messages to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_message(user_id, message):\n",
    "    \n",
    "    # Load the conversation history\n",
    "    conversation_messages = get_conversation_history(user_id)\n",
    "\n",
    "    # Store the new message\n",
    "    store_message(user_id, message, \"user\")\n",
    "    \n",
    "    # Add the new message to the conversation\n",
    "    conversation_messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{ \"text\": message }]\n",
    "    })\n",
    "\n",
    "    # Send the conversation to the LLM\n",
    "    model_response = bedrock.converse(\n",
    "        modelId=\"amazon.nova-lite-v1:0\",\n",
    "        messages=conversation_messages,\n",
    "        system = [{ \n",
    "            \"text\": \"Please provide a helpful, conversational response based on the available information and conversation history.\" \n",
    "        }],\n",
    "        inferenceConfig = {\n",
    "            \"maxTokens\": 300,\n",
    "            \"temperature\": 0.7,\n",
    "            \"topP\": 0.9\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Parse the message from the LLM, store it and return it\n",
    "    assistant_msg = model_response['output']['message']['content'][0]['text']\n",
    "    store_message(user_id, assistant_msg, \"assistant\")\n",
    "\n",
    "    return conversation_messages, assistant_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Johny! It's great to hear that you enjoy skiing and pickleball. Given your interests, I think you might enjoy trying out some other sports that combine physical activity, teamwork, and a bit of strategy. Here are a few suggestions:\n",
      "\n",
      "1. **Ice Hockey**: Since you live in Montreal, ice hockey is a popular sport and a great way to stay active during the winter months. It's a fast-paced game that requires good coordination and teamwork.\n",
      "\n",
      "2. **Soccer**: Soccer is a fantastic sport for improving your agility, endurance, and overall fitness. Montreal has a vibrant soccer community, and there are plenty of leagues and clubs you can join.\n",
      "\n",
      "3. **Basketball**: If you enjoy a sport that involves quick movements and strategic plays, basketball could be a great fit. It's a high-energy game that can be played both indoors and outdoors.\n",
      "\n",
      "4. **Tennis**: Given your interest in pickleball, tennis might be another sport you'd enjoy. It offers a great cardiovascular workout and helps improve hand-eye coordination and reflexes.\n",
      "\n",
      "5. **Swimming**: If you're looking for a low-impact sport that's great for overall fitness, swimming is an excellent choice. It's a full-body workout that's easy on the joints.\n",
      "\n",
      "6. **Rugby**: For those who enjoy a sport with a strong team element, rugby could be a fun option. It's a physically demanding game that emphasizes strength, speed, and\n"
     ]
    }
   ],
   "source": [
    "user_id = \"johny\"\n",
    "convo, response = send_message(user_id, \"My name is Johny and I live in Montreal. I like skiing and playing pickle ball. What other sport do you recommend I do?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You mentioned earlier that you live in Montreal. If you're referring to a different city, please let me know, and I can provide more tailored recommendations based on your location. Enjoy exploring new sports and activities!\n"
     ]
    }
   ],
   "source": [
    "convo, response = send_message(user_id, \"What city do I live in?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(convo, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
