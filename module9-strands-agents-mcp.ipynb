{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building conversational applications with Strands Agents\n",
    "\n",
    "In this notebook, you learn how to use Strands agents to integrate external capabilities into conversational applications.\n",
    "\n",
    "This is a conversion of the original module9-converse_with_tools.ipynb that used the Converse API with tool specifications. Instead, we'll use Strands agents for a more streamlined approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "In this task, you set up your environment and install the required packages for Strands agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from strands.models.bedrock import BedrockModel\n",
    "from mcp import stdio_client, StdioServerParameters\n",
    "from strands import Agent\n",
    "from strands.tools.mcp import MCPClient\n",
    "\n",
    "model_id = \"amazon.nova-lite-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the MCP Server command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MCP server using stdio transport\n",
    "stdio_mcp_client = MCPClient(lambda: stdio_client(\n",
    "    StdioServerParameters(\n",
    "        command=\"uv\", \n",
    "        args=[\"run\", \"module9-mcp_server.py\"]\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the MCP Client and create the Strands Agent\n",
    "\n",
    "Now we create a Strands agent that uses the Bedrock LLM and our defined tools. The agent will automatically handle tool calling and conversation flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the Bedrock LLM wrapper for Strands\n",
    "llm = BedrockModel(\n",
    "    model_id=model_id\n",
    ")\n",
    "\n",
    "# Create an agent with MCP tools\n",
    "with stdio_mcp_client:\n",
    "    # Get the tools from the MCP server\n",
    "    tools = stdio_mcp_client.list_tools_sync()\n",
    "\n",
    "    # Create an agent with these tools\n",
    "    agent = Agent(\n",
    "        model=llm,\n",
    "        tools=tools,\n",
    "        system_prompt=\"You are a helpful assistant that can perform calculations and search for information on DuckDuckGo.\"\n",
    "    )\n",
    "\n",
    "    # Send the question\n",
    "    agent(\"What is Amazon SageMaker? What is launch year multiplied by 2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
